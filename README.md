# Home_Sales
This project demonstrates the use of SparkSQL to analyze and process home sales data. Using PySpark, the project explores various queries and optimizations, including creating temporary views, caching, partitioning data, and analyzing the average home prices based on various conditions.

The goal of this project is to explore a dataset of home sales, perform various SQL-like queries using PySpark, and optimize query performance using caching and partitioning. Key steps include:

1. Loading and processing data using PySpark.
2. Creating temporary views for data analysis.
3. Performing SQL queries on the data.
4. Caching tables to improve query performance.
5. Partitioning data and working with parquet files.
6. Uncaching and verifying data integrity.
